% Encoding: UTF-8

@WWW{Dickson2018,
  author = {Dickson, Andrew},
  title  = {A.I. and the Art of Spotting Fakes},
  year   = {2018},
  date   = {2018-07-20},
  urldate = {2019-04-01},
  url    = {https://medium.com/s/story/a-i-and-the-art-of-spotting-fakes-6a674b0bdfef},
}

@WWW{Elgammal2017,
  author = {Elgammal, Ahmed},
  title  = {Picasso, Matisse, or a Fake? A.I. for Attribution and Authentication of Art at the Stroke Level},
  year   = {2017},
  date   = {2017-11-16},
  url    = {https://medium.com/@ahmed_elgammal/picasso-matisse-or-a-fake-a-i-for-attribution-and-autehntication-of-art-at-the-stroke-level-f4ec329c8c26},
}

@WWW{Pim2018,
  author = {Pim, Harrison},
  title  = {A new kind of image search},
  year   = {2018},
  date   = {2018-10-18},
  url    = {https://stacks.wellcomecollection.org/a-new-kind-of-image-search-5870c2cdadcb},
}

@WWW{Pim2018a,
  author = {Pim, Harrison},
  title  = {Exploring Wellcome Collection with computer vision},
  year   = {2018},
  date   = {2018-10-17},
  url    = {https://stacks.wellcomecollection.org/exploring-wellcome-collection-with-computer-vision-7513dff8126d},
}

@WWW{Moriarty2018,
  author = {Moriarty, Adam},
  title  = {Computers Colouring the Collections},
  year   = {2018},
  date   = {2018-07-23},
  url    = {https://medium.com/aucklandmuseum/computers-colouring-the-collections-a32054295b1e},
}

@WWW{Moriarty2018a,
  author = {Moriarty, Adam},
  title  = {AI and Museum Collections},
  year   = {2018},
  date   = {2018-07-23},
  url    = {https://medium.com/@adamrmor/ai-and-museum-collections-c74bdb724c07},
}

@WWW{Drygalska2018,
  author = {Drygalska, Ewa},
  title  = {Museum Treasures - AI at the National Museum in Warsaw},
  year   = {2018},
  date   = {2018-10-12},
  url    = {https://medium.com/new-technologies-in-museum/museum-treasures-ai-at-the-national-museum-in-warsaw-e6cfc50329ab},
}

@WWW{Knott2017,
  author = {Knott, Jonathan},
  title  = {Using AI to analyse collections},
  year   = {2017},
  date   = {2017-02-14},
  url    = {https://www.museumsassociation.org/museum-practice/artificial-intelligence/14022017-using-ai-to-analyse-collections},
}

@WWW{ING2016,
  author = {ING},
  title  = {The Next Rembrandt},
  year   = {2016},
  url    = {https://www.nextrembrandt.com/},
}

@Online{Sood2016,
  author = {Sood, Amit},
  title  = {Every piece of art you've ever wanted to see - up cloase and searchable},
  year   = {2016},
  url    = {https://www.ted.com/talks/amit_sood_every_piece_of_art_you_ve_ever_wanted_to_see_up_close_and_searchable/up-next},
}

@WWW{Diagne2017,
  author = {Diagne, Cyril and Gaël, Hugo},
  editor = {Experiments with Google},
  title  = {Tags. Could computers help identify artworks?},
  year   = {2017},
  url    = {https://experiments.withgoogle.com/tags},
}

@WWW{PhysicsBlog2014,
  author   = {{The Physics arXiv Blog}},
  title    = {When a machine learning algorithm studied fine art paintings , it was things art historians had never noticed},
  year     = {2014},
  date     = {2015-08-18},
  url      = {https://medium.com/the-physics-arxiv-blog/when-a-machine-learning-algorithm-studied-fine-art-paintings-it-saw-things-art-historians-had-never-b8e4e7bf7d3e},
  subtitle = {Artificial intelligence reveals previously unrecognised influences between great artists},
}

@WWW{Rowe2017,
  author = {Rowe, Paul},
  title  = {Looking at Sarjeant Gallery’s collection through robot eyes},
  year   = {2017},
  date   = {2017-10-31},
  url    = {https://medium.com/@armchair_caver/looking-at-sarjeant-gallerys-collection-through-robot-eyes-c7fd0281814e},
}

@TechReport{Elgammal2018,
  author      = {Elgammal, Ahmed and Mazzone, Marian and Liu, Binchen and Kim, Diana and Elhoseiny, Mohammed},
  title       = {The Shape of Art History in the Eyes of the Machine},
  institution = {Rutgers University and College of Charlston},
  year        = {2018},
}

@TechReport{Sabatteli2018,
  author      = {Sabatteli, Matthia and Kestemont, Mike and Daelemans, Walter and Geurts, Pierre},
  title       = {Deep Transfer Learning for Art Classification Problems},
  institution = {Université de Liège and Universiteit Antwerpen},
  year        = {2018},
}

@WWW{Guo2017,
  author = {Guo, Yufeng},
  title  = {The 7 Steps of Machine Learning},
  year   = {2017},
  date   = {2017-08-31},
  url    = {https://towardsdatascience.com/the-7-steps-of-machine-learning-2877d7e5548e},
}

@WWW{Guo2017a,
  author = {Guo, Yufeng},
  title  = {What is Machine Learning?},
  year   = {2017},
  date   = {2017-08-25},
  url    = {https://towardsdatascience.com/what-is-machine-learning-8c6871016736},
}

@WWW{Stewart2019,
  author = {Stewart, Matthew},
  title  = {Simple Introduction to Convolutional Neural Networks},
  year   = {2019},
  date   = {2019-02-27},
  urldate = {2019-03-01},
  url    = {https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac},
}

@TechReport{Zhao2017,
  author      = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark en Ordonez Vicente and Chang, Kai-Wei},
  title       = {Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints},
  institution = {University of Virginia and University of Washington},
  year        = {2017},
}

@WWW{Tallon2019,
  author    = {Tallon, Loic},
  editor    = {{The Metropolitan Museum of Art}},
  title     = {Sparking Global Connections to Art through Open Data and Artificial Intelligence},
  year      = {2019},
  date      = {2019-02-04},
  url       = {https://www.metmuseum.org/blogs/now-at-the-met/2019/met-microsoft-mit-art-open-data-artificial-intelligence},
  abstract  = {Ambitie: MET collectie moet de meest toegankelijke, zichtbare en bruikbare collectie zijn op het internet. Educatie en disseminatie en kennis geven over de collectie buiten het fysieke. Daarom werden high res beelden en metadata van kunstwerken vrijgegeven onder een public domain CreatIve Commons Zero licentie.

Resultaten:
- gebruik van third party platforms om de toegang tot de collectie te verhogen
- ook de reach op Wikimedia werd verviervoudigd
- nieuwe manieren om de collectie te begrijpen
- duurzame dateintegraties met andere websites
- inspiratie voor nieuwe kunstvormen
- zowel stijging van downloads en pageviews van de collectie op de Met's website
- gelijkaardige programma's werden gelanceerd door andere musea

Keyword dataset:
- ontworpen op de collectie. beantwoord de vraag "wat zie je?", bv. mannen, vrouwen, portretten, bloemen, dansen
- deze keywords kunnen gebruikt worden om te leren hoe een onderwerp weergegeven wordt doorheen periodes en culturen.
- deze keywords hebben ervoor gezorgd dat de kunstwerken op een andere manier geanalyseerd kunnen worden --> kunnen gebruikt worden als trainingdata.
--> MET x Microsoft x MIT
- bieden een zicht op hoe AI gebruikt kan worden om kunst tot bij het publiek te brengen.

Volgende stappen:
- subject dataset wordt geïntegreerd in de museumwebsite om de browsing ervaring het publiek te verbeteren en mogelijkheid te geven om de filteren op keyword
- AI introduceren van prototype tot product
- integratie met Wikidata via een SDC project.

},
  urldate = {2019-03-10},
}

@WWW{Lih2019,
  author       = {Lih, Andrew},
  title        = {Combining AI and Human Judgment to Build Knowledge about Art on a Global Scale},
  year         = {2019},
  date         = {2019-03-04},
  url          = {https://www.metmuseum.org/blogs/now-at-the-met/2019/wikipedia-art-and-ai},
  organization = {The Metropolitan Museum of Art},
  abstract     = {200 vrijwilligers (jongeren, kusthistorici, technologists, ...) kwamen samen in het MET om kunstwerken te classificeren. Dit om een AI systeem te ondersteunen. Metadata werd verzameld op Wikidata om zo de beschrijvende metadata beschikbaar te maken.

Nut? Musea bewaren metadata van hun collectie (beschrijvende, locatie, provenance...) Deze metadata zijn niet gestandaardiseerd.

In 2018 starte MET met een subject-keyword tagging project.  --> consistente beschrijvende metadata creëren die beschikbaar is voor iedereen onder een open licentie. In een Hackaton werd er gezocht naar het gebruik van AI hiervoor --> prototypes ontwikkelen.

Kunnen de keywords gebruikt worden om een ML model te trainen that accuraat tags kon voorspellen voor kunstwerken.

METs subject keywords werden geconnecteerd met Wikidata en beelden werden geprocessed. Goede resultaten op het vlak van landschapsachtige schilderijen (bomen, bergen, dieren). Daarna een soort van spel om feedback te geven aan de software.

Integratie met Wikidata omdat het een semantische databank is. als je zoekt naar zoogdier krijg je zowel honden, katten etc. + multilangual.},
  urldate    = {2019-03-10},
}

@WWW{MMA2019,
  editor    = {{The Metropolitan Museum of Art}},
  title     = {The Met x Microsoft x MIT},
  year      = {2019},
  url       = {https://www.metmuseum.org/about-the-met/policies-and-documents/open-access/met-microsoft-mit},
  abstract  = {MET, Microsoft ne MIT werkten samen in een hackathon om te zien hoe AI kon helpen om mensen met kunst te connecteren. Hiervoor werden de beelden, data en keyword dataset gerbuikt.
Doel: Inbeelden en ontwikkelen van nieuwe schaalbare manieren om het wereldwijde publiek de meest bekende kunstcollectie te laten ontdekken, leren en ermee te creëren via AI.

Microsoft - ingenieurs en Gebruikte technologie: voorgebouwde API's zoals Azure Cognitive Services, conversational AI en Azure Machine Learning.
MIT - studenten van MIT Open Learning en the Knowledge Futures Group
MET - curatorial staff, digital staff en researchters

Prototypes:

* Artwork of the Day: maakt gebruik van Microsoft AI om een kunstwerk te vinden die past met vandaag. een kunstwerk dat relevant is met de context van wereldgebeurtenissen en met je huidige omstandigheden. presoonlijk, geen een persoon zal hetzelfde kunstwerk op dezelfde dag krijgen. door er een API van te maken, kan je het ook integreren in alarm clocks, frigo's, etc.

* Gen Studio, maakt ook gebruik van Microsoft AI om je visueel en creatief te laten navigeren door de shared features en dimensies van the MET's Open Access collection. Een tapijt van ervaringen gebaseerd op gesofisticeerde generatieve adversarial netwerks (GAN) om zo de collectie te ontdekken. Kunstenwerken worden op nieuwe manieren met elkaar gecombineerd gebaseerd op stijlen, materialen en vormen in de MET collectie en ongekende kunstwerken ontdekken die dezelfde visuele karakteristieken delen. Een immersieve visuele ervaring. We envision Gen Studio becoming a far-reaching platform that develops appreciation of the immense historical depth and scale of The Met collection, and which identifies new perspectives on the visual relationships between cultures and the production of individual artworks.

* My Life, My Met: maakt gebruik van AI om je instagram feed in een kunstwerk om te zetten. Het gebruikt Microsoft AI om je posts op instagram te analyseren te te vervangen door het closest matching beeld uit de open access artworks van de MET collection. Zo wordt kunstwerk gebracht in dagelijkse interacties van je leven.

* Storyteller: maakt gebruik van Microsoft AI om een kunstwerken te kiezen uit de MET collection die de verhalen illustreren die je zou willen vertellen. Hiervoor maakt het gebruik van voice recognition AI om de discussie te volgen en kunstwerken te delen die passen bij de verhalen die verteld worden.

* Tag, That's It!: combinatie van mensen en machines om zo de toegankelijkheid van de MET collection te verhogen voor de miljoenen mensen op de Wikimedia platformen. Een vorm van crowdsourcing waarbij de keywords die gegenereerd worden door het AI model gefinetuned kunnen worden.  Het AI model kan zo verbeterd worden en kan toegepast worden op iedere museumcollectie.
},
  urldate = {2019-03-10},
}

@WWW{Vu2018,
  author    = {Vu, Kevin},
  title     = {Beginner’s Guide: Image Recognition And Deep Learning},
  year      = {2018},
  date      = {2018-11-29},
  url       = {https://dzone.com/articles/beginners-guide-image-recognition-and-deep-learnin},
  urldate = {2019-03-21},
  abstract  = {Deep learning en neural networks: algoritmes die slimmer worden in verloop van tijd.
deep learning is een geavanceerd veld in ML en het hedendaagse wonder van AI.

ML: de computer krijgt input en op basis daarvan doet het voorspellingen. Bij deep learning gaat het anders te werk en heeft het te maken met de tijd die het krijgt.

neurale netwerken gebruiken algoritmes die in lagen naast elkaar liggen. Ieder algoritme is daarom afhankelijk van de resultaten van de omliggende algoritmes. het probeert een proces te creëren dat overeenkomt met de manier waarop mensen redeneren. in het geval van image recognition spreekt met van convolutional neural networks.

CNN: beelden worden opgedeeld in nummers. Wanneer we iets in het echt zien, dan geven ons hersenen het zin door het te labelen, voorspellen en het herkennen van specifieke patronen. Bij CNN doet men dat ook, maar dan met nummers. Convolution bestaat uit het gebruiken van twee functies die een derde functie opleveren. een CNN merget verschillende sets van informatie saemn en poolt die samen om zo een accurate representatie van een beeld te verkrijgen. Daarna wordt het beld beschreven in verschillende data die doior een neuraal netwerk gebruikt kan worden om een voorspelling te doen over wat het is. Computers kunnen dan die voorspelling gebruiken voor andere applicaties.

Een neuraal netwerk leert over de tijd heen of de voorspellingen accuraat zijn.

Er gaat veel menselijk werk in om een AI goed te krijgen. Het is vooral belangrijk om een goede dataset te hebben. De dataset wordt gebruikt om te trainen om dan het model in te zetten in het wild. Er zijn goed ontwikkelde datasets, bv. ImageNet. Dit bestaat uit 3.2 miljoen gelabelde beelden. Deze dataset kunnen door AI modellen gebruikt worden om op te oefenen.

ImageNet werd opgevolgd door AlexNet, dat een CNN architectuur gebruikte dat nog steeds in gebruik is.},
  urldate = {2019-03-10},
}

@WWW{Simonite2018,
  author = {Simonite, Tom},
  title  = {When It Comes to Gorillas, Google Photos Remains Blind},
  year   = {2018},
  date   = {2018-01-11},
  url    = {https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/},
}

@WWW{Olafenwa2018,
  author = {Olafenwa, Moses},
  title  = {Object Detection with 10 lines of code},
  year   = {2018},
  date   = {2018-06-16},
  url    = {https://towardsdatascience.com/object-detection-with-10-lines-of-code-d6cb4d86f606},
}

@WWW{nullN2018,
  editor    = {{The New School}},
  title     = {Parsons MS Data Viz Partners with The Met to Visualize Their Digital Collection},
  year      = {2018},
  date      = {2018-10-25},
  url       = {https://blogs.newschool.edu/news/2018/10/parsons-ms-data-viz-partners-with-the-met-to-visualize-their-digital-collection/},
  abstract  = {Met: kunst van over 5000 jaar en 470.000 objecten online en 406.000 publieke domein beelden.

Door het Open Access programma kunnen ook andere mensen gebruik maken van de collectie, wat de Parsons School of Design gedaan heeft. Studenten hebben visualisaties gedaan met de dataset.},
  urldate = {2019-03-10},
}

@WWW{Simonite2017,
  author = {Simonite, Tom},
  title  = {Machines Taught by Photos Learn a Sexist View of Women},
  year   = {2017},
  date   = {2017-08-21},
  url    = {https://www.wired.com/story/machines-taught-by-photos-learn-a-sexist-view-of-women/},
}

@WWW{Raval2017,
  author = {Raval, Siraj},
  title  = {YOLO Object Detection (TensorFlow tutorial)},
  year   = {2017},
  date   = {2017-11-15},
  url    = {https://youtu.be/4eIBisqx9_g},
  urldate = {2019-03-21}
}

@WWW{Gong2017,
  author       = {Gong, Kevin},
  title        = {Best Practices for Custom Models in Watson Visual Recognition},
  year         = {2017},
  date         = {2017-11-13},
  url          = {https://medium.com/ibm-watson/best-practices-for-custom-classifiers-in-watson-visual-recognition-1015a273f75d},
  organization = {IBM Watson},
  urldate      = {2019-03-05},
  abstract     = {trainingsbeelden moet overeenkomen met de beelden die je wil laten analyseren. Duidelijke visuele verschillen tussen trainingsbeelden en testbeelden zorgen voor slechte resulaten.

factoren:
- resolutie
- belichting
- hoek
- focus
- kleur
- vorm
- afstand tot onderwerp
- aanwezigheid van andere objecten

ideaal: 100+ beelden per concept

waar hebben visuele API's het moeilijk mee:
- herkennen van mensen (face recognition)
- details detecteren --> beter dan om het beeld op te breken in stukjes of om in te zoomen op de relevante onderdelen
- classificatie van emoties },
  keywords     = {training},
  urldate    = {2019-03-05},
}

@TechReport{Elhassouny2017,
  author      = {Elhassouny, Azeddine and Tam, Le Nhan and Sayed, Dina and Steffens, Bjoern and Sri, Lak},
  title       = {Building Cognitive Applications with IBM Watson Services: Volume 3 Visual Recognition},
  institution = {IBM Redbooks},
  year        = {2017},
  type        = {techreport},
  file        = {:Elhassouny2017 - Building Cognitive Applications with IBM Watson Services_ Volume 3 Visual Recognition.pdf:PDF},
  urldate   = {2019-03-08},
}

@TechReport{Caimotti2017,
  author      = {Caimotti, Emanuele and Montagnuolo, Maurizio and Messina, Alberto},
  title       = {An Efficient Visual Search Engine for Cultural Broadcast Archives},
  institution = {Politecnico di Torino and RAI Radiotelevisione Italiana},
  year        = {2017},
  type        = {resreport},
  urldate     = {2019-03-05},
  file        = {:Caimotti2017 - An Efficient Visual Search Engine for Cultural Broadcast Archives.pdf:PDF},
}

@WWW{Brownlee2017,
  author    = {Brownlee, Jason},
  title     = {What is the Difference Between Test and Validation Datasets?},
  year      = {2017},
  date      = {2017-07-14},
  url       = {https://machinelearningmastery.com/difference-test-validation-datasets/},
  urldate   = {2019-03-21},
  abstract  = {training data: gebruik je om het model te trainen
validation data: gebruik je om te controleren hoe goed je model is en om de paramters te finetunen (geen overfit etc.)

validatiedata en testdata zijn steeds verschillend. validatiedata mag niet gebruikt worden om het model te trainen.

waarom validatie? model valideren met trainingsdata geeft partijdige resultaten.

train test split approach:
- data wordt willekeurig opgesplitst in twee delen: training en validatie
- model wordt getraind met trainingsdata
- model wordt gebruikt om de resultaten te voorspellen van de validatiedata
- de foutenmarge op de validatiedata geeft een schatting van de test error rate

model moet geëvalueerd worden op basis van data die niet gebruikt werd om het model te testen of te finetunen. wanneer er een grote hoeveelheid data beschikbaar is, dan moet er data opzijgezet wordne om het finale model te evalueren.

training data --> data om het model te creëren
validatie data --> data om de kwaliteit van het model te evalueren

russel & norvig zeggen dat de training ook verder opgesplitst kan worden in een training en validatieset. en dat de validatieset, subset van trainingset, kan gebruikt worden om al vroeger in het proces de mogelijkheden van het model in te schatten.

– Training set: A set of examples used for learning, that is to fit the parameters of the classifier.
– Validation set: A set of examples used to tune the parameters of a classifier, for example to choose the number of hidden units in a neural network.
– Test set: A set of examples used only to assess the performance of a fully-specified classifier.
},
  urldate = {2019-03-21},
}

@WWW{MMA2017,
  editor    = {{The Metropolitan Museum of Art}},
  title     = {The Tagging Initiative},
  year      = {2017},
  url       = {https://www.metmuseum.org/about-the-met/policies-and-documents/open-access/tagging-initiative},
  abstract  = {het onderwerp van ieder kunstwerk wordt geïdentificeerd en er wordt een tag voor ieder kunstwerk gemaakt.
==> op deze manier worden de kunstwerken ontsloten voor het brede publiek en zijn ze beter doorzoekbaar.

onderwerpen zij beschikbaar onder CC0 licentie en downloadbaar via een CSV en beschikbaar voa de MET Collection API. Deze dataset bevat common woorden die informatie geven over het onderwerp van het kunstwerk en die het zoeksystemen eenvoudiger maken om content op te halen: titel, kunstenaar, medium, nationaliteit, etc.},
  urldate = {2019-03-10},
}

@Proceedings{Mele2017,
  title     = {Proceedings of the AI*CH 2017},
  year      = {2017},
  date      = {2017-11-14},
  editor    = {Mele, Francesco and Origlia, Antonio and Sorgente, Antionio},
  subtitle  = {The 11th workshop onArtificial Intelligence for Cultural Heritage},
  eventdate = {2017-11-14},
  url       = {http://smcm.isasi.cnr.it/AIxCH2017/wp-content/uploads/2017/12/Proceedings_AIxCH2017.pdf},
  urldate   = {2019-03-05},
  file      = {:Mele2017 - Proceedings of TheAI_CH 2017.pdf:PDF},
}

@Misc{darknet13,
  author       = {Redmon, Joseph},
  title        = {Darknet: Open Source Neural Networks in C},
  year         = {2013-2016},
  urldate = {2019-03-31},
  howpublished = {\url{http://pjreddie.com/darknet}},
}

@WWW{Brownlee2019,
  author = {Brownlee, Jason},
  title  = {A Gentle Introduction to Computer Vision},
  year   = {2019},
  date   = {2019-03-19},
  urldate   = {2019-03-21},
  url    = {https://machinelearningmastery.com/what-is-computer-vision/},
}

@Misc{wikiCV,
  author       = {{Wikipedia contributors}},
  title        = {Computer vision --- {Wikipedia}{,} The Free Encyclopedia},
  year         = {2019},
  howpublished = {\url{https://en.wikipedia.org/w/index.php?title=Computer_vision&oldid=896283892}},
  urldate         = {2019-06-15},
}

@Report{Blessings2013,
  author      = {Blessings, Alexander and Wen, Kai},
  title       = {Using Machine Learning for Identification of Art Paintings},
  type        = {resreport},
  institution = {Stanford University},
  year        = {2013},
  pagetotal   = {5},
  abstract    = {Machine learning applications have been suggested for many tasks. We have investigated the suitability of applying machine learning to the problem of art identification, which we believe to be a new, but promising field.  Our approach focuses  on  classifying  works  of  seven  different  artists,  by using a multi-class SVM with state-of-the-art features. Our results indicate that machine learning has good potential to classify art works. We conclude this paper by analyzing our results.},
  keywords    = {paintings},
}

@InProceedings{Mensink2014,
  author    = {Mensink, T. E. J. and van Gemert, J. C.},
  title     = {The Rijksmuseum Challenge: Museum-Centered Visual Recognition},
  booktitle = {ACM International Conference on Multimedia Retrieval},
  year      = {2014},
  file      = {MensinkICMIR2014.pdf:https\://ivi.fnwi.uva.nl/isis/publications/2014/MensinkICMIR2014/MensinkICMIR2014.pdf:PDF},
}

@WWW{UniAntwerpen2017?,
  editor   = {{Universiteit Antwerpen}},
  title    = {INSIGHT},
  year     = {2017},
  url      = {http://uahost.uantwerpen.be/insight/index.php/about/},
  urldate  = {2018-12-08},
  abstract = {INSIGHT is a research project that targets the digital assets of two museum clusters in Brussels: Royal Museums of Fine Arts of Belgium and Royal Museums of Art and History. This project aims to deploy the recent advances in Artificial Intelligence (language technology and computer vision in particular) to support the enrichment of these collections with descriptive metadata. An important focus of this project is the issue of transferring knowledge from open collections, such as The Rijksmuseum Dataset, to other players in the field. To this end, we investigate issues relating to multimodality or the way in which we can simultaneously model different information streams about digital heritage objects (e.g. in different languages, or across different media). Apart from multimodality, multilinguality will be another crucial aspect of our research, which is of course important in the context of federal heritage collections in Belgium. The end goal of this project is to develop and release a series of practical Machine Learning tools for managing digital collections. A major outcome of this project will be an export of the digital collections involved as a “Europeana-ready” linked open data set, which will contribute to the broader accessibility of these collections.},
  comment  = {stand van zaken},
}

@Misc{WikiImageNet,
  author       = {{Wikipedia contributors}},
  title        = {ImageNet --- {Wikipedia}{,} The Free Encyclopedia},
  year         = {2019},
  howpublished = {\url{https://en.wikipedia.org/w/index.php?title=ImageNet&oldid=900080629}},
  urldate         = {2019-06-26}
}

@Article{Russakovsky2014,
  author       = {Russakovsky, Olga and Deng, Jia and Su, Heo and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander and Fei-Fie, Li},
  title        = {ImageNet Large Scale Visual Recognition Challenge},
  journal      = {International Journal of Computer Vision},
  journaltitle = {International Journal of Computer Vision - September 2014},
  year         = {2014},
}

@Article{Pokharna2016,
  author    = {Pokharna, Harsh},
  title     = {For Dummies - The Introduction to Neural Networks we all need ! (Part 1)},
  journal   = {TechnologyMadeEasy},
  year      = {2016},
  date      = {2016-07-26},
  url       = {https://medium.com/technologymadeeasy/for-dummies-the-introduction-to-neural-networks-we-all-need-c50f6012d5eb},
  urldate = {2019-06-16},
}

@WWW{Serrano2017,
  author    = {Serrano, Luis},
  title     = {A friendly introduction to Convolutional Neural Networks and Image Recognition},
  year      = {2017},
  date      = {2017-03-20},
  url       = {https://www.youtube.com/watch?v=2-Ol7ZB0MmU},
  urldate = {2019-06-16},
}

@WWW{Brownlee2019a,
  author = {Brownlee, Jason},
  title  = {A Gentle Introduction to the ImageNet Large Scale Visual Recognition Challenge (ILSVRC)},
  year   = {2019},
  date   = {2019-05-01},
  urldate   = {2019-06-21},
  url    = {https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/},
}

@WWW{Lardinois2018,
  author    = {Lardinois, Frederic},
  editor    = {TechCrunch},
  title     = {Google’s AutoML lets you train custom machine learning models without having to code},
  year      = {2018},
  date      = {2018-01-17},
  url       = {https://techcrunch.com/2018/01/17/googles-automl-lets-you-train-custom-machine-learning-models-without-having-to-code},
  urldate = {2019-06-17},
}

@Unpublished{Vanstappen2019,
  author = {Vanstappen, Henk},
  title  = {VR4CH: Visual recognition voor erfgoedcollecties. Eindrapport},
  year   = {2019},
}

@WWW{Dickson2019,
  author = {Dickson, Andrew},
  editor = {OneZero},
  title  = {A.I. Will Enhance - Not End - Human Art},
  year   = {2019},
  date   = {2019-03-29},
  urldate = {2019-04-01},
  url    = {https://onezero.medium.com/a-i-will-enhance-not-end-human-art-f575e9ff9325},
}

@WWW{Haskiya2019,
  author = {Haskiya, David},
  editor = {Europeana Pro},
  title  = {How to set up a generous interface prototype in less than a day},
  year   = {2019},
  date   = {2019-04-09},
  urldate = {2019-04-01},
  url    = {https://pro.europeana.eu/post/how-to-set-up-a-generous-interface-prototype-in-less-than-a-day},
}

@WWW{Derveaux2019,
  author    = {Derveaux, Alexander},
  editor    = {FOMU},
  title     = {Beeldherkenning in de registratiepraktijk},
  year      = {2019},
  url       = {https://www.fotomuseum.be/collectie/onderzoek0/projecten/Beeldherkenning_in_de_registratiepraktijk.html},
  urldate = {2019-06-22},
}

@WWW{Clairbot2019,
  author    = {Clairbot},
  editor    = {Clarifai},
  title     = {General FAQ | Clarifai Help Center},
  year      = {2019},
  url       = {http://help.clarifai.com/articles/687603-general-faq},
  urldate = {2019-06-24},
}

@WWW{ClarifaiGeneral,
  author    = {Clarifai},
  editor    = {Clarifai},
  title     = {General Model | Clarifai},
  year      = {n.d.},
  url       = {https://www.clarifai.com/models/general-image-recognition-model-aaa03c23b3724a16a56b629203edc62c#documentation},
  urldate = {2019-07-02},
}

@WWW{Heemkunde,
	author = {{Heemkunde Vlaanderen}},
	title  = {Huis van Alijn zoekt taggers (m/v) voor beschrijven van anonieme snapshots},
	year   = {2011},
	date   = {2011-11-07},
	urldate = {2019-08-11},
	url    = {https://www.heemkunde-vlaanderen.be/huis-van-alijn-zoekt-taggers-mv-voor-beschrijven-van-anonieme-snapshots/},
}

@WWW{ClarifaiAPI,
  author = {Clarifai},
  editor = {Clarifai},
  title  = {API Guide},
  year   = {n.d.},
  url    = {http://developer-dev.clarifai.com/developer/guide/},
  urldate = {2019-07-02}
}

@WWW{Tsang2018,
  author    = {Tsang, Sik-Ho},
  editor    = {Coinmonks},
  title     = {Review: ZFNet - Winner of ILSVRC 2013 (Image Classification)},
  year      = {2018},
  date      = {2018-08-19},
  url       = {https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103},
  urldate = {2019-07-02},
}

@WWW{ClarifaiBlog,
  author = {Clarifai},
  title  = {How to create \& add a custom computer vision model to your app in just a few minutes},
  year   = {2019},
  date   = {2019-01-28},
  url    = {https://medium.com/@clarifai/how-to-create-add-a-custom-computer-vision-model-to-your-app-in-just-a-few-minutes-267fcc7aecf6},
}

@Unpublished{Lievens2017,
  author   = {Lievens, Stijn},
  title    = {Artificiële Intelligentie},
  year     = {2017},
  subtitle = {Lesnota's},
  note     = {Ongepubliceerde cursus},
}

@Online{MOMA2018?,
	author       = {MOMA},
	editor       = {MOMA},
	title        = {Identifying art through machine learning},
	year         = {2018},
	url          = {https://www.moma.org/calendar/exhibitions/history/identifying-art},
	subtitle     = {A project with Google Arts \& Culture Lab},
	organization = {Museum of Modern Art},
	urldate      = {2018-12-08},
	abstract     = {Given years of experience and some diligent research, identifying each work of art in an old exhibition photo doesn’t sound so hard, does it? Now imagine you have tens of thousands of photos, dating back to 1929. MoMA’s Digital Media team and Google Arts & Culture Lab set out to face this daunting challenge—or at least get a head start—using machine learning and computer vision technology.},
	comment      = {stand van zaken - achtergrond},
	keywords     = {MOMA, Google Cloud Vision, Google Arts & Culture Lab},
}

@Online{Nasjonalmuseet2017?,
	author   = {Nasjonalmuseet},
	editor   = {{Nasjonalmuseet}},
	title    = {Project: "Principal Components"},
	year     = {2017},
	url      = {http://www.nasjonalmuseet.no/en/collections_and_research/collection_management/digital_collection_management/Project%3A+%C2%ABPrincipal+Components%C2%BB.b7C_wJjU4L.ips},
	urldate  = {2018-12-08},
	abstract = {In this project, we have tried out artificial intelligence in principal component analysis on our images, by using neural networks and algorithms.
	
	Two of the results in the project are described in more detail below.
	
	The algorithms that show compositional similarities for us in a new user interface on our website.
	The algorithm that classifies the National Museum's art by subject keywords.},
	keywords = {Iconclass, Caffe, gelijkenissen, gezichten, kleuren},
}

@Misc{Gatz2016,
	author       = {Gatz, Sven},
	title        = {Conceptnota aan de Vlaamse Regering. Naar een duurzame cultureel-erfgoedwerking in Vlaanderen.},
	year         = {2016},
	date         = {2016-03},
	subtitle     = {Een langetermijnvisie voor cultureel erfgoed en cultureel-erfgoedwerking in Vlaanderen},
	organization = {Vlaamse Regering},
	keywords     = {cultuurbeleid},
}

@WWW{Koehrsen2018,
  author    = {Koehrsen, Will},
  editor    = {Towards Data Science},
  title     = {Beyond Accuracy: Precision and Recall},
  year      = {2018},
  date      = {2018-03-03},
  url       = {https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c},
  urldate   = {2019-08-06},
  keywords  = {machine learning, accuracy},
  timestamp = {2019-08-13},
}

@Comment{jabref-meta: databaseType:biblatex;}
